# Student's T-Distribution
Woha! Student's? What the freak? 
Actually it has the story behind it, well which I don't think I should write here, but it is about the person who presented this theory wanted to present anonymously so he wrote "*student*" instead of his name. 

### Why it exists?
Recall. In the hypothesis testing, I mentioned this ↓
> #### $$ z = \frac{\bar x - \mu}{\frac{\sigma} {\sqrt{n~}}} $$
> **NOTE**: This ↑ method assumes that we know the population standard deviation ($\sigma$). If ***we don't know*** the std, then we need to perform the **T-Test** — about which we will learn later.

And also if you can recall much back in the file of "standardization" *(Conversion.md)*, I didn't mentioned that but... still in that formula we needed to know the standard deviation to make it work.

Now, it is the time when we will learn about it. Here, we have the sample, but the standard deviation of the data is not present. 

> ### Using the t-table, the Student's T-test determines if there is a significant difference between two sets of data.

*Due to variance and outliers it is not enough to compare mean values. T-test **also considers** the <U>variance of samples.</U>*

**Two Types of t-tests**:
1. ***One-Sample Test***: Tests the null hypothesis that the population mean is equal to a specified value $\bar X$ on a sample mean $\bar x$.
	- <u>Example</u>: want to check if sample of students have the same mean test score as the population of the students.
2. ***Independent Two-Sample Test***: Tests the null hypothesis that two sample means $\bar x_1$ and $\bar x_2$ are equal.
	- <u>Example</u>: Want to check the test score of two separate samples of students have a statistically significant difference. <br> <br>You take 2 samples, check the average between 2 groups, there is gonna be some difference. But to check the significance, we use this Independent Two-Sample Test.

3. ***Dependent Paired-Sample Test***: It is used when the samples are dependent. 
	- One sample tested twice (repeated measurements)
	- Two samples have been matched or paired.<br> <br>
	- <u>Example</u>: Want to check if the same group of students has improved results on test  scores before the prep course and after the course.


In the Z statistic, we required the standard deviation - which was **called z statistic.** Here we will call it **t statistic**.

## For One Sample Test

## $$ t = \frac{\bar x - \bar X}{\frac {SE_p} {\sqrt{n~}} }$$

$\bar x = \text{Sample Mean}$
$\bar X = \text{Population Mean}$
$SE_p = \text{Standard Error}$
$n = \text{Sample Size}$

*As we don't know the population std, we are gonna use the $SE_p$*. 
- Then, like z-table we use t-table
- The t-score depends on:
	- Degress of freedom (based on the sample size $n$)
	- Chosen significance level (default `0.05`)

**Compare t-score**
# $$t \lessgtr t_{n - 1, ~\alpha}$$
- $t = \text{t-statistic}$
- $t_{n - 1, ~\alpha} = \text{t-critical}$
- $n - 1 = \text{degrees of freedom}$
- $\alpha = \text{significance level}$
- $\lessgtr ~= \text{Lessthan Greater than}$
- $\lesseqgtr ~= \text{Less than equal to Greater than}$
- $\gtreqless ~= \text{Greater than equal to Less than}$

## For Independent Two-Sample Test
The calculation differs slightly.
- <u>**Equal** sample size</u> with <u>**Equal** Variance</u> *(Rarest)*
- <u> **Unequal** sample size</u> with <u>**Equal** Variance</u> *(Rare)*
- <u> **Equal / Unequal** sample size</u> with <u> **Unequal** variance</u> *(Common)*

> When working with the 2 samples t-test, it is often useful to think of the t-test *as a **ratio*** of **Signal** to **Noise**.
> <br>
> Signal = Sample Mean
> Noise = Sample Variance

### $$ t = \frac {\text{signal}} {\text{noise}}$$
### $$ ~~~~~~~~~~~~~~~~~~~~~~~~~~ = \frac {\text{difference in means}} {\text{sample variability}}$$
## $$~~~~~~~~~~~~~=\frac{| \bar x_1 - \bar x_2 |}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} $$

Then, as before
**Compare t-score**
# $$t \lessgtr t_{df,~\alpha}$$
- $t = \text{t-statistic}$
- $t_{df, ~\alpha} = \text{t-critical}$
- $df = \text{degrees of freedom}$
- $\alpha = \text{significance level}$

→ ***Berfore this we had only one sample to work with, so the degrees of freedom worked out by onlu $n -1$. But now, we are dealing with 2 samples, so the degrees of freedom calculation should have to be done.***

### General Formulae to Calculate DF
## $$df = \frac{\left( \frac{s_1^2}{n_1} +  \frac{s_2^2}{n_2}\right)} {\frac{1}{n_1 - 1} \left( \frac{s_1^2}{n_1}\right) + \frac{1}{n_2 - 1} \left( \frac{s_2^2}{n_2}\right)} $$

Wow! Amazing one! 
It is not complex. If you try to have a look at it, the same elements from the numerator are used in the denominator and divided by their respective $n$. That's it.

**THAT FORMULAE IS TO BE USED WHEN YOU <U>DON'T HAVE SAME VARIANCES</U>**.

But when you have same variance, the **same formulae can be used** but you can use the **shorter version**.

### $$df = (n_1 -1) + (n_2 - 1)$$
### $$↓$$
### $$df = n_1 + n_2 - 2$$

Hah! Funny! ***But we don't have to worry, we can simply use the $df = \frac{\left( \frac{s_1^2}{n_1} +  \frac{s_2^2}{n_2}\right)} {\frac{1}{n_1 - 1} \left( \frac{s_1^2}{n_1}\right) + \frac{1}{n_2 - 1} \left( \frac{s_2^2}{n_2}\right)}$ formula every time because we are gonna use programming!***

![T-dist vs z-dist](https://i.imgur.com/zP8HP8H.png)

The `t-dist` has **fatter tail**. As the degree of freedom increases, the t-dist gets similar to the z-dist (normal dist).

> And a thing is, while dealing with the t-distribution, it works well if the sample size is 30 or lower. If the sample size gets higher, t-dist becomes more like z-dist, so then we would have to use the z-table instead of t-table.

Check this [cool video on t-test and t-values](https://www.youtube.com/watch?v=pTmLQvMM-1M).

# Alright!
Next file, we will go through an example. See you there ∞