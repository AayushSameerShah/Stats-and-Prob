# ANOVA
*(A girlish name which has a strong influence in statistics)*

### ANOVA â€” Analysis of Variance

## Why and What
Previously, we learnt about the `z distribution` and `t distribution` which we used to answer the question / problem like:
> What is the probability that two samples come from the same population?

There, we have the normal distribution (z) and quite like normal distribution with fatter tails (t).

**Here**, in ANOVA we will have something strange and different kind of distribution. Called: ***F - Distribution***. 

We will use it to answer the question like:
> What is the probability 	that two samples come from populations that have the same variance?

**Also**, we will use it to answer the question like:
 > What is the probability that ***three or more*** samples come from the same population?

See the difference between those 2 questions. In the first we are comparing ***2 populations*** and in the second one we are comparing ***3 or more samples*** and answering whether they are from the same population or not.

**So, the advantage**, here we can use 3 or more samples instead of 1 or 2 as we were able to previously.
___
<font size=6><b>N</b></font>ow, ANOVA by hand.

**A quick note:** The calculation shown below is not to do by hand, you know, you are the programmer, you can make a class in python an make this work done in seconds, ***but*** for the sake of clarity in mind of **what is going on?** in ANOVA, I need to show the process of calculation.

As we know it, ANOVA can also be worked out for 3 or more samples. For such we need to calculate the `f score`. With that information in mind, let's consider the following example â†“

$\text{grp}_1 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$
$\text{grp}_2 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$
$\text{grp}_3 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$

The main target for us to achieve is to see **whether they are from the same population or not** and if they are, they **will have the same mean**.

Thus, we can state our hypothesis as *(Ignoring the general steps)*:
$H_0 : \bar x_1 =\bar x_2 = \bar x_3$

***NOW***, <u>let's pretend</u> that we **don't know anything** about ANOVA. Now, without ANOVA how are we going to proceed?

The general procedure that we would follow *could* be:
$H_0 : \bar x_1 = \bar x_2$ *($\alpha$ = 0.05)*
$H_0 : \bar x_2 = \bar x_3$ *($\alpha$ = 0.05)*
$H_0 : \bar x_3 = \bar x_1$ *($\alpha$ = 0.05)*
Where, we will check the hypothesis 3 times for **each pair** of the variables. Will the work be done? *(Assuming we are using `t` or `z` score method here as we don't know ANOVA yet.)*

**Nah.**
Even if we have worked out by the method above, and even if the accuracy set `95%` for each, we can't say in the final result to reject or fail to reject hypothesis as **our final accuracy *won't* be 95%**.

**How?**
Because, we have used the `95%` 3 times, which is $0.95 \times 0.95 \times 0.95 = 0.857 = 85.7\%$.

So after 3 tests, instead of `95%`, our overall accuracy would be only `85.7%`. Which is not fine. 

***It is where, ANOVA comes in***.
Now, forget about the individual checks, ANOVA handles them in a different way *(still remember, our aim / hypothesis is the same $H_0 : \bar x_1 =\bar x_2 = \bar x_3$)*.

Here, we will compute an **F-Value** and then we will compare it with our **Critical Value** determined by our *degrees of freedom*. Yes, here also we will need the degrees of freedom and based on the F-Table, we will get the critical value.

*(New thing I realized now is, in z-table we could directly get the p value from z or z from z, but here we will need the extra degrees of freedom to get some value, which is fine.*

*If you didn't understand what I have just â†‘ said, please ignore it â€” safely ignore it.)*

Continuing with the same data:
$\text{grp}_1 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$
$\text{grp}_2 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$
$\text{grp}_3 = \#_1,~\#_2,~\#_3,~...~,~\#_{10}$

We need an **F** value. The formula is ***ridiculously*** simple.

# $$F = \frac{\frac{SSG} {df_{\text{group}}}} {\frac{SSI} {df_{\text{internal}}}}$$

$SSG$ = The Sum of Square of whole groups' mean with each group's individual mean. *(The following calculation will clear this up)*

$SSI$ = The Sum of Square of each group's mean with their elements themselves. *(A same part we calculate in calculation of variance)*

$df_{\text{group}}$ = Degrees of Freedom for the group. Calculated by:
$$df_{\text{group}} = n_\text{groups} - 1$$

$df_{\text{internal}}$ = Degrees of Freedom for the elements. Calculated by:
$$df_{\text{internal}} = (n_\text{rows} - 1) \times n_\text{groups}$$

**All formulae are extremely, insanely easy**. Let's have a hands on.
___
**General Steps** when you are calculating with ANOVA

**Step 1**: Calculate the mean of each group
Here, for our groups we have got â†“

$\text{mean}(\text{grp}_1) = \bar x_1 = 44$
$\text{mean}(\text{grp}_2) = \bar x_2 = 50$
$\text{mean}(\text{grp}_3) = \bar x_3 = 53$

**Step 2**: Calculate the mean of all groups' mean
$\bar x_\text{total} = \frac{\bar x_1 ~+~ \bar x_2 ~+~ \bar x_3}{n}$

$~~~~~~~~~= \frac{44 ~+~ 50 ~+~ 53}{3}$

$~~~~~~~~~= 49$

**Step 3**: Calculate Sum of Squares for Groups (***SSG***)
$SSG = \sum(\bar x_\text{total} - \bar x_g)^2 \times n_\text{rows}$
$~~~~~~~~~=[(49 ~-~ 44)^2 ~+~ (49 ~-~ 50)^2 ~+~ (49 ~-~53)^2] \times 10$
$~~~~~~~~~=420$


**Step 4**: Calculate the degrees of freedom for groups ($~df_\text{group}~$)
As the formulae says:
$df_{\text{group}} = n_\text{groups} - 1$
$~~~~~~~~~~~~ = 3 - 1$
$~~~~~~~~~~~~= 2$

$\therefore df_\text{group} = 2$

**Step 5**: Calculate the Sum of Square for internals
$SSI = \sum(\bar x_g - x_{gi})^2$ 
$~~~~~~~~ = \sum(\bar x_1 - x_{1i})^2 + \sum(\bar x_2 - x_{2i})^2 + \sum(\bar x_3 - x_{3i})^2$
$~~~~~~~~ = 3300$

Easy, just find the $(\bar x - x)^2$ for all groups and sum them up.

**Step 6**: Calculate the degrees of freedom for internals($~df_\text{internals}~$)
As the formulae says:
$df_{\text{internal}} = (n_\text{rows} - 1) \times n_\text{groups}$
$~~~~~~~~~~~~~~~ = (10 - 1) \times 3$
$~~~~~~~~~~~~~~~= 27$

**DONE!**
Now, just plug the numbers in! We have,
$SSG = 420$
$SSI = 330$
$df_{\text{group}} = 2$
$df_{\text{internals}} = 27$

$F = \frac{\frac{SSG} {df_{\text{group}}}} {\frac{SSI}{df_{\text{internal}}}}$

$~~~~=\frac{ \frac{420}{2} }{ \frac{3300}{27} }$

$~~~~= 1.718$

$\therefore F = 1.718$

 *(Note: These are not the steps in hypothesis testing, those are the same, just when you have 3 sets you will follow the following to calculate ANOVA â€” That's why I kept the steps in **bold** instead of <u>underline</u> which are reserved for the steps of hypothsis ðŸ˜‰)*.
___

We have the F-value. Now, as the `z` and `t` distribution we have the `f` distribution. Which, well will be then become our critical value.

## The F-distribution
![F dist](https://i.imgur.com/6nFIEAa.png)

The shaded area is when the critical value is `0.05`, of course it will change as the our alpha changes.

### The table
![F table](https://i.imgur.com/hkihi5V.png)
- The same thing is followed here, as we have the confidence level of 0.05, we will look at such table which has the values of 0.05.
- Then we have **2** as $df_\text{group}$
- Then we have **27** as $df_\text{internal}$

Thus, we have got our **critical value** `3.35`.

Getting the critical value in python
```python
from scipy import stats
stats.f.ppf(1 - 0.05, dfn=2, dfd=27)
>>> 3.35
```
The way you pass the confidence is `1 - 0.05` so keep that in mind.

Done? Yes!
Let's check it in our hypothesis $H_0$.
$H_0 : \bar x_1 = \bar x_2 =  \bar x_3$

But, here we are looking at the direct distribution. So instead of direct comparison, we will look at our distribution.

![Decision](https://i.imgur.com/sINYXoO.png)

Here, $1.717 < 3.35$ means we will **Fail to Reject** the null hypothesis.



# Aha!
In the next file we will use the knowledge with some examples!