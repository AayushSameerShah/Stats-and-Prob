# Distributions
This section is gonna be a cool one, as I have previously tried to understand the distributions, but mostly I have had skipped to learn about that as there were many of them like *poisson*, *gaussian*, etc... but now it seems I will get the idea of what they **acutally** are and why do we care to use them.

Let's see how ∞
___
> **Distribution**: describes all of the probable outcomes of the variables.
- In **discrete** distribution the sum of all individual probability ***must*** equal to 1. (It is discrete)
- In **continuous** distribution the the *area under the curve* equals 1.

### There are some types of distributions
1. **Discrete probability distribution**
	1.1 Uniform distribution
	1.2 Binomial distribution
	1.3 Poisson distribution (પોસાન)
2.  **Continuous probability distribution**
	2.1 Normal distribution
	2.2 Exponential distribution
	2.3 Beta distribution


## 1. Discrete Probability Distribution
It is also called: "**Probability Mass Function**" 
As discussed, it has 3 distributions namely:
1) Uniform
2) Binomial 
3) Poisson
Let's check them one by one.
*(I am actually feeling that I am writing too much now, which I should not, but still it is a good thing.)*

### 1.1 Uniform Distribution
> In this distribution all outcomes of the event are **equiprobable** and are **discrete**.

For an example, 
If you roll a "paasa", then it has all equiprobable and discrete outcomes. You can't roll 2.4 in the paasa. We can only roll 1, 2, 3, 4, 5 or 6. Thus is the discrete and all values are having same probability of happening 0.167.

Now if we make a chart ↓
![Uniform Dist](https://i.imgur.com/MGNWqxo.png)
Thus, all outcomes are having the same probability and in turn, as the rule says "**All probability should add up to 1**" ← satisfy as well.

### 1.2 Binomial Distribution
> **Binomial**: there are <u>*two discrete*</u>, and <u>*mutually exclusive*</u> outcomes of a trial.

Now the term "Bi" = 2 = Two.
Hence, when we are talking about the binomial distribution, we are **talking about the 2 discrete outcomes**. Like:
- Head / Tail
- Yes / No
- Male / Female
- On / Off
- **Success** / **Failure**

The last one was written in **bold** I know. It is because when we are talking about the "Binomial", we are talking about the "Bernoulli Trials" and in the Bernoulli Trials we define the terms as Success or Failure. So in the cases like above, we will call **one of them as success** and the second one will be failure.

> **Bernoulli Trial** is a random experiment in which there are only 2 possible outcomes. Success or Failure.

There are 2 key points that have to be checked to verify that it is the Bernoulli Trial.

1. **The probability of success `p` is constant**.
    `Let's say we consider the *flipping a coin* event as a Bernoulli trial, then we have to choose of of the outcomes as the success. Let's say we chose HEADS as success. So, it is constant. It is going to be 0.5 no matter what. The probability is constant of the success.`
    
2. **Trials are independent of one another**.
   `There should not be any effect of the first trial on the future trials. And here we see that it is.`

Now, we are meeting these 2 conditions, hence we can call it "Bernoulli trial".

**Did you NOTE?** In the very first distribution 1.1 Uniform, we had the *equiprobable* outcomes. Now, here the 2 of the outcomes ***can have different probability***. 

*(We took the example of head and tails which are indeed equiprobable, but still it is for you to note that, we have the option to include Non-equiprobable outcomes.)*

Now, what is the goal?
The thing that we want to achieve with this kind distribution is that, **If I do the trials N many times, what is probability of my success M times out of N times?**

See, this is simple thing. It can be done by hand. 
↓ ***Please read whole — neatly explained*** ↓

**Example**: Let's say again, we are doing for the `0.5` chanced coins for head and tail. Success is the Head. So, We can calculate, `out of 10` trials the probability of success only 1 (and only 1) time. If you try to find it, it is low. **Why?** because the probability of success is 0.5 and out of 10 if you want 1 and only 1 time head then the probability is low.

Then check the probability of success 2 times (only 2) out of 10. The probability will be bigger than that one getting the 1 out of 10. Do for 3, 4, 5... the probability will increase. **At 5** probability of getting heads out of 10 will be highest. Because it is 0.5. Then if you calculate for 6, 7, 8... the probability will decrease. Because you are wanting all 6 time heads, 7 times heads out of 10 flips when you have the probability of 0.5.
—
Now, being an analyst (and viz lover) you plot the distribution like below ↓
![Binomial Distribution](https://media0.giphy.com/media/L0t7LkNzCI6ZhmdCKA/giphy.gif?cid=790b7611e12dae2169ac22d979fb1e0bd46b736135d621d3&rid=giphy.gif&ct=g)

Thus, it shows if the possibility of success is 0.5 and you flip the coin 50 times getting 25 heads out of 50 has the highest possibility.

We can too have different success let's say **getting less than 2** in the paasa. So here, our success is getting less than 2. Hence the probability...

$P(\text{success}) = \frac{2}{6} = \frac{1}{3}$

$P(\text{failure}) = \frac{4}{6} = \frac{2}{3}$
So, then if we make the distribution, it will give us the different distribution which is concentrated in the center where it the number of times **N** trials are made and the number of probability we have here $\frac{1}{3}.$ 

In the first example we had $\frac{1}{2}$ prob of success so out of 10 trials, our distribution centered at 5. If (let's say) the proability of heads were $\frac{1}{3}$ then our distribution would have centered around number 3, instead of 5 ($\frac{10}{3} \approx 3$).

It is simple, if you read it again. I have got it now.

#### How are we gonna calculate the stuff?
Okay, we know that there came the distribution, but HOW? What calculation do we have to perform to get such?

Well, actually, we can get it by simply multiplying the probabilities but, here we have another **pre—made** formulae for us.
### 
<font size=5> Which we call: **"Binomial Probability Mass Function"** </font>
## $$ P(x: n,p) = ~_nC_x \times p^x \times (p')^{(n - x)} $$
<center> ↓ </center>

## $$ P(x: n,p) = ~_nC_x \times p^x \times (1 - p)^{(n - x)} $$

How easy! Now, let's understand the practical implementation.

<b>Problem:</b> "I am throwing the paasa for `16` times in a row. What is the probability of getting number `5` three times out of those `16` trials?

<b>Solution:</b>
$n = \text{trials} = 16$
$p = \text{success} = \frac{1}{6}$
$x = \text{how many we want} = 3$

Hence,
$P(x: n,p) = ~_nC_x \times p^x \times (1 - p)^{(n - x)}$ 

$P(3: 16, \frac{1}{6}) = ~_{16}C_3 \times \left(\frac{1}{6}\right)^3 \times (1 - \frac{1}{6})^{(16 - 3)}$ 

$~~~~~~~~~~~~~~~~~~~~~= 560 \times 0.00463 \times (0.833)^{13}$ 
$~~~~~~~~~~~~~~~~~~~~~= 0.241$ 

In python:
```python
from scipy.stats import binom
binom.pmf(m, n, prob)
```

So, the bottom line that I would like to say is, 
> Binomial distribution is a discrete probability distribution of the trial with 2 events and shows the probability of success when M is some number $\#_1$ under N trials which again is a number $\#_2$. <br> — <br> The  Binomial Mass Function $P(x: n,p)$ is used to calculate the success probability.

That's it for the binomial, next up we will see the poisson.

### 1.3 Poisson Distribution (પોસાન)
> **Poisson** distribution considers the number of successes <u>*per unit of time*</u> over the course of many units.

This distribution is **very similar** to the binomial, as here we also calculate for the success and failure. But the **main difference** lies between these two is *how we measure the probability*. 

**In the binomial**,
We consider out of N trials M success's probability. 

**In this poisson**,
We consider number of successes per some continuous unit.

Things will get more clear as we move along, but the thing till now to understand is only that difference. Here the number of trials is not much important. 

***(Keep in mind that poisson distribution is not just to calculate for <u>unit of time only</u> but it can be calculated for <u>any other continuous units</u> like distance, height etc.)***
___
As binomial, here too we have the mass function.

<font size=5> Which we call: **"Poisson Probability Mass Function"** </font>

## $$P(x) = \frac{\lambda^x \times e^{-\lambda}}{x!}$$

And to get the $\lambda$, go for the equation below ↓

## $$\lambda = \frac{n ~~\text{occurences}}{\text{interval}} = \mu$$
That function is like the *objects per people*. It is like, `If I eat 60 chocolates, how many have I ate per second? Then it is 60 / 60 = 1 chocolate per second`.

Here, the $\text{interval}$ ***is*** the **continuous** variable. Unlike the binomial we had discrete number of trials *(as of course we can not do 5.3 trials)*.

**How does it look like?**

![Poisson Dist](https://media0.giphy.com/media/9lGB26BPwRaL9NtUBp/giphy.gif?cid=790b7611d87e91de2d6cf66182f24cd28e79c57419e81b74&rid=giphy.gif&ct=g)

See in the viz above. It is much like the one we did in binomial, but the things are **concentrated on the *mean* ($\mu$)**. In the binomial we had the highest probability where the the probability of the success was met $\#$ times but here... let me explain with an example.

**Example**:
I am a Digital Sherlock who observes the patterns. 
- Once I sat on the bridge and observed the cars passing by the road.
- I sat there for an hour.
- I counted all cars and at the end of the hour I counted total 600 cars.

**Now, if my mind asks me *"How many cars passed per minute?"***

Then I would simply go and do the math $\frac{600}{60} = 10$.  So then, I concluded that per minute 10 cars pass by the road on that baker street. 

—

**Now, again my mind asks me *"If 10 cars per minute can pass by the road, what is the probability of passing 20 cars per minute by the road?"***

I am impressed with my brain. So I opened my books and found the *binomial probability mass function* which is used to calculate the probability on what-if situation... but there I observed, that I am given unit of time and here we are **not doing any trials**, so I decided to go with ***Poisson Probability Mass Function***. 
 
 The main function: $P(x) = \frac{\lambda^x \times e^{-\lambda}}{x!}$ and that requires the lambda. 

So: $\lambda = \frac{n ~~\text{occurences}}{\text{interval}} = \mu$ which I have already figured out.

Wow! Now I have things which I needed to solve the problem. So here it goes:

→ $~\lambda = \text{average cars per unit of time} = 10$
→ $~x = \text{when cars are 20} = 20$
→ $P(x) = \frac{\lambda^x \times e^{-\lambda}}{x!} = \text{probability of 20 cars when given mean the time interval}$

<br>

So, $P(x) = \frac{\lambda^x \times e^{-\lambda}}{x!}$

$~~~~~P(20)=\frac{10^{20} \times e^{-10}}{20!}$

$~~~~~~~~~~~~~~~~= \frac{100000000000000000000 \times 0.00000454}{2432902008176640000}$

$~~~~~~~~~~~~~~~~\approx0.0001866$

Then **I will reply** to my mind that, *"Bro, if the mean value is such 10 then how can you possibly say that there will be 20 cars! There can be 11 cars or 9 cars which will have the probability of $0.11373$ and $0.12511$ respectively and of 10 cars  $0.12511$ (yes, the same as 9)*

Now before my mind asks another question. I find the probability of cars from 0 to 100 to satisfy my mind. And doing such will give the chart like above. Which we have just shown.

See, if we change the interval like instead of per minute, we can do per 15 minutes, 2 seconds etc... and the *distribution will change* likewise.

#### Cumulative mass function
Note that, in the previous examples my mind was asking some ***discrete*** questions like: *What is the probability of getting 20 cars?*  — Here it means ***exactly 20***.  <u>But what if</u>, my mind starts asking: *What is probability of more than 20 cars?*

Now here, in this situation we will have to use the **Cumulative Mass Function**. It is simply the **sum of all discrete probs** (upto some certain point.)

Here we can set "*More than 20 cars = 20 to 25 cars*". So we would...

### $$P(x:x \leq 25) = P(20) + P(21) + P(22) + P(23) + P(24) + P(25)$$

And, don't worry — the **summing up the probabilities won't exceed 1** as we know the sum of all discrete prob is 1 so if we are calculating probs, it will be equal to 1 always for all natural numbers combined.
___
**COMMON**: If we are asked for the probability of *at least 1 event* to occur. It means $1 - P(\text{none})$ event occur. So,

$1 - \frac{\lambda^x \times e^{-\lambda}}{x!} = 1 - e^{-\lambda}$

**THE ASSUMPTION**: The poisson distribution assumes that ***the probability of success is STEADY over the interval***. Which means if I am seeing the car for an hour, and the average per minute is 10 then it means on an average per minute it should be 10. It should not be like all car rushes on after 50 minutes and before that the average of car was 2. See? If that is such, the un-even distribution of cars, this poisson <u>**will not work**</u>. 

And thus, we can sub-divide the intervals in the smaller and smaller time frames (if it is steady).

**ONE MORE THING**: If the statement above said is satisfied of mean stability, then **we can sub-divide** to mean in the smaller units. Again, if and only if the stability is maintained. 

**Ex**: We've got the average of 10 cars per minute. Now from that data if I want to calculate the **average cars per second** then all I have to do is to divide by the interval $\frac{10}{60} = 0.1667$ cars per second.

In python:
```python
from scipy.stats import poisson

# On exact x
poisson.pmf(x, mu)

# Fewer than x (cumulative)
poisson.cdf(x, mu)
```

## 2. Continuous Probability Distribution
Recall that, the Discrete Probability Distributions were also called: "*Probability Mass Function*".
—
As said here, the Continuous Probability Distributions also have another name: "**Probability Density Function**".

As noted above:
1) Normal Distribution
2) Exponential Distribution
3) Beta Distribution

***Acutally***, Normal Distributions are very useful in the business and are mostly applicable in the studies. The rest of 2 are not that much important. The course has skipped those so will I too. Here, we will focus on only Normal one.

### 2.1 Normal Distribution
- Many real life variable follow the normal distributions
	- People's height and weights
	- Test scores
	- Measurement errors

Now, remember we are in the zone of **continuous** variables. So, the things will change a little bit.

> The variable whose values tend to be around a central value with <u>no bias left</u> or <u>right</u> and gets *close* to the normally distributed shape. Hence, called normal distribution.

Aka: Gaussian = Bell Curve = Normal.

"*Unlike the discrete distribution where the sum of all discrete probs equals to 1, here the **area under the curve** will equal to 1.*"
___
Some properties:
- The probability of **any** <u>specific outcome</u> is **0**.
- We can **only** find probabilities between some <u>specific interval</u>.
- Mean, Median and Mode are same.

Now, the main person is called a person who has 3 properties and are in common, which we have just shown above. Now as in person there are 2 genders Male and Female - here too we have 2 types.

1. Standard Normal Distribution
2. Normal Distribution

Yes, the term Normal Distribution is itself confusing but hey, we can sometimes call "Man" as a whole person - to address all human being on the planet and in person we have Man and Woman, so something like that.

Now,
#### 1. Standard Normal Distribution
I like to call this distribution as a "***perfectly predictable distribution***". 
The Standard Normal Dist has:
1. Mean = Median = Mode = 0
2. Standard Deviation = 1

![Standard Dist](https://media1.giphy.com/media/OCaLlkF27ks4QeHaWI/giphy.gif?cid=790b761128746017fb4138dff0d5447d32f2a1c5782f01d2&rid=giphy.gif&ct=g)

It follows other properties which is common for normal distribution as well which are:
1. Mean $\pm$ 1 STD covers: `68.27%` values.
2. Mean $\pm$ 2 STD covers: `95.45%` values.
3. Mean $\pm$ 3 STD covers: `99.73%` values.

Now, these are really very useful properties to know. They will help us for the *hypothesis testing* in the upcoming notebooks.

#### 2. Normal Distribution
I would like to call it ***"Non-standard Normal Distribution"***

As unlike the standard distribution, it doesn't follow the pre-set values of mean, meadian, mode and standard deviation. Indeed, it is the normal distribution so, it will have the shape like bell curve and will follow the common properties like Mean = Median = Mode... but here the MMM can be anything and std can be anything too.

![Std vs Normal](https://i.imgur.com/GUpvA3V.png)

See in the image above. There is the green distribution which shows the normal distribution. The standard will always have $\text{mean = median = mode = 0}$ ***and*** $\sigma = 1$. While the normal can have $\text{mean = median = mode} = \#_1$ ***and*** $\sigma = \#_2$.

Indeed it will also follow all other properties like:
1. Mean $\pm$ 1 STD covers: `68.27%` values.
2. Mean $\pm$ 2 STD covers: `95.45%` values.
3. Mean $\pm$ 3 STD covers: `99.73%` values.

Just except, it can have any value for MMM and Std.
___
Now, just for fun — I want to show a function which can be used to draw the normal distribution by giving a number

### $$ f(x) = \frac{1}{\sqrt{2 \pi\sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$

Or just copy and paste this ↓ in python which will give you the normal dist for the given value:
```python
def get_value(x):
    return (1 / np.sqrt(2 * np.pi * sigma**2)) * (np.exp(-(x - mean)**2) / (2 * sigma**2))
```

# Actually, That's it.
With that said, we should cover up the *types of distributions* part of this book, now next we will see the z-scores for normal distribution. **Why** and **How**.
