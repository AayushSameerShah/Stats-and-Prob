 # Regression
Yeah, it is greatly repeated topic for me, but it is still exciting to learn and reinforce it. **Regression**, what is it? Let's not talk much as I told you in the introduction but will only note things which are related.

> ***General Definition***: The goal of regression is to develop and equation or formula that best describes the relationship between variables.

If I were asked to develop a definition:
> ***My Definition***: Regression is the way to find relationship between independent variable X and dependent variable Y such that given X we can estimate the value of Y. 

This indeed is not a baby definition since I got used to interpret and understand the definitions like this now, but still it is.

- Now, we find the best fit line by the formulae.
- We actually are going to minimize the error.
- The general form $\hat y = \beta_0 + \beta_1x_1$.
- $\beta_1$ can be -ve, +ve or 0.

Formulae for $\beta_1$:
 - $\beta_1 = r \cdot \frac{s_y}{s_x}$

- $\beta_1 = \frac{\sum (x - \bar x)(y - \bar y)}{\sum (x - \bar x)^2}$

Formulae for $\beta_0$:
- $\beta_0 = 	\bar y - \beta_1 \bar x$

#### Don't directly rely on the regression calculation — Visualize it first
This might sound obvious, I mean I do always, but for the sake of an example see the dataset **"Anscome's Quartet"** which has four different combination of x and y. If we get the best fit line with this linear regression it will be same for them all but not the best for all. See ↓

![Anscombes Quartet](https://i.imgur.com/pqLCcUA.png)

**NEW**:
If the line is best - fit, and the dataset is *good to go* for linear regression, then ***plotting each residuals on the plot should make a <u>normal distribution</u>*** *(more or less)*.

Which means you are more or less getting the **same** error for most of the points and slowly getting negative error for some points and positive error for some points.

###  Multiple Linear Regression
Here, we will try to estimate y from multiple features (x). 
Then, the formulae becomes:
$$ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n$$

Which means: *$\beta_n$ reflects the change in $\hat y$ for a given change in $x_n$ all else **remaining constant.***

**NEW**
The formulae for regression learnt in grass was WRONG. Here is the formulae style if you don't want to use the matrix. 

If there are 2 features:
$$\beta_1 = \frac{\sum(x_2 - \bar x_2)^2 \sum(x_1 - \bar x_1)(y - \bar y) ~-~ \sum(x_1 - \bar x_1)(x_2 - \bar x_2) \sum(x_2 - \bar x_2)(y - \bar y)}{\sum(x_1 - \bar x_1)^2 \sum(x_2 - \bar x_2)^2 ~-~ (\sum(x_1 - \bar x_1)(x_2 - \bar x_2))^2}$$

$$\beta_2 = \frac{\sum(x_1 - \bar x_1)^2 \sum(x_2 - \bar x_2)(y - \bar y) ~-~ \sum(x_1 - \bar x_1)(x_2 - \bar x_2) \sum(x_1 - \bar x_1)(y - \bar y)}{\sum(x_1 - \bar x_1)^2 \sum(x_2 - \bar x_2)^2 ~-~ (\sum(x_1 - \bar x_1)(x_2 - \bar x_2))^2}$$

$$\beta_0 = \bar y - \beta_1 \bar x_1 + \beta_2 \bar x_2$$


### AWFUL!
That's why we use the matric formulae. 

### Notice In Multiple Regression
<font size=6><b>T</b></font>here  can be features which correlate together. The standard definition goes that, 

> $\beta_n$ reflects the change in $\hat y$ for a given change in $x_n$ all else **remaining constant.**

Now, if both variables are correlating together, we can't adjust one **without affecting another**.  It is called ***MULTICOLLINEARITY***. 

# Done!
Next up, we will see the Chi-Square. The most exciting part. See you there ∞