# Central Tendency
Core of Centrals:
> **What is the average return?**<br> — *Measure of Central Tendency*

> **How far from the average did individual values stray?** <br> —*Measure of Dispersion*

<font size=6><b>T</b></font>he measures of central tendency mainly are MMM.
- Mean = *Calculated Average*
- Median = *Middle value*
- Mode = *Most occuring value*

→  They show some sort of the middle location of the data, in the required axis.
→  They ***fail to*** describe the "Shape" of the data.

# Dispersion
**Three ways** to measure the dispersion RVS.
- **Range**
	- $Range = max - min$

- **Variance**
	- $\sigma = \frac {\sum(x - \bar x)^2} {n}$
	*Overall in average, how far the points are from the mean*
	
- **Standard Deviation**
	- $\text{std} = \sqrt{\sigma}$
___
### <center> NEW
When you see: <font size=6>$\frac{...}{n}$</font>  vs <font size=6>$\frac{...}{n - 1}$</font> the $n-1$ refers to **Bessel's Correction**. 

```The bassel's correction tries to reduce the bias due to some sort of finite sample size. We are trying to estimate the population measurement. We by making `n - 1` we are making the overall measurement larger.```

### <center> NEW 
___

We generally use the **Standard Deviation** for the study. When we use the variance, we get the $\text{units}^2$ which doesn't get much interpretable. So to make it intuative, we have to use the std. Then we can say: *"values lie one standard deviation of the mean"*.

# Quartiles
The advantage is that, we don't have to use any kind of aggregation of the data, like we did before. Here, the quartile works on the *position* of the data point. 

We show the quartiles in the form of a **Box Plot** which is so information packed plot. It also covers the outliers. Now, to find the outliers we have to know a term — "Fence". 

**Fence**: It is the 1.5 times of the width of the IQR. Anything outside of the fence is the *outlier*. ***You should call Fence → Threshold***. 

The cool thing about the fence is that, we are **NOT** calculating the fence as the 10% of data or those which are below 2% likewise are the outliers, we are **actually** calculating it by using all data. 

$$ IQR = Q_3 - Q_1$$	

So now, 
$$
outlier = x > Q_3 + IQR \times 1.5 \\ 
outlier = x < Q_1 - IQR \times 1.5
$$

# Bi-variate Data
*It is when you compare 2 variables*.

<font size=6><b>W</b></font>hen the bi-variate data comes into the picture, we usually try to find the correlation. It is the time ***when we need to be aware that we don't assume the "causation" for the sake of they both are trending together***.
—
Example, if there is the question *"Does the increase of police officer, decrease the number of crimes?"*

To answer that, we can look at the correlation between the rates of those variables and then ***to determine the causality*** we will need to look at other sources of context and information and further statistical analysis to understand whether or not there is the *cause and effect* relationship between 2 variables.

### Covariance
**A common way** to compare 2 variables is **to compare their variances** — How far from each item's mean do typical values fall?

We can't directly compare, we need to *normalize* the data first. 

$$ 
cov = \frac {\sum (x - \bar x) (y - \bar y)} {n}
$$
The `cov` is essentially looks like the variance, but it is for 2 variables together.  Covariance can be *positive or negative*. 

We can't generally interpret the covariance as it doesn't show **how much** or on what degree do they trend, but we can see the **sign** and determine the **type of relationship.** If the covariance is closer to 0 then there is no correlation and the further the covariance is, the strengthen the relation between them is.

### Pearson Correlation
It will normalize the covariance so that both datasets become comparable. The general formula is:

$$
corr = \frac{cov} {S_x \times S_y}
$$
<center> ↓ </center>

$$
corr = \frac{\sum (x - \bar x)(y - \bar y)} {\sqrt{(x - \bar x)^2} \times \sqrt{(y - \bar y)^2}} 
$$

# Next up
We will talk about the probability (amazing thing that I love), see you there!
